#!/usr/bin/env python
# encoding: utf-8
"""
@author: dstch
@license: (C) Copyright 2013-2019, Regulus Tech.
@contact: dstch@163.com
@file: light_boosting_classify.py
@time: 2019/2/18 16:30
@desc:
"""

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import lightgbm as lgb
import xgboost as xgb
import time, datetime
from sklearn import *

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os

print(os.listdir("../input"))

train_data = pd.read_csv('../input/train.csv')
test_data = pd.read_csv('../input/test.csv')


def value_count(df):
    col_count = dict(df.value_counts())
    return len(col_count)


# 处理缺失值，计算每列缺失值比例，并对应进行处理
data_list = []
for i in train_data.columns:
    d = len(train_data) - train_data[i].count()
    r = (d / len(train_data)) * 100
    rate = '%.2f%%' % r
    # print('字段名为：',str(i).ljust(10),'缺失值数量:',str(d).ljust(4),'缺失数量占比：',rate,'值个数：',value_count(train_data[i])) #这里print主要是为了在脚本中观察是否获取到想要的数据，方便调试。
    data_list.append([str(i).ljust(10), str(d).ljust(4), rate, value_count(train_data[i]), train_data[i].dtype])
missing_data_summary = pd.DataFrame(data_list,
                                    columns=['column_name', 'missing_data_count', 'missing_data_rate', 'value_count',
                                             'value_type'])
# sort for missing value rate
missing_data_summary.sort_index(axis=0, level=None, ascending=False, inplace=False, kind='quicksort',
                                na_position='last', sort_remaining=True, by=['missing_data_rate'])

# sort for missing value rate
missing_data_summary.sort_index(axis=0, level=None, ascending=False, inplace=False, kind='quicksort',
                                na_position='last', sort_remaining=True, by=['missing_data_rate'])

# remove rate>50% data
drop_index = [28, 41, 8, 68, 52, 71, 75]
train_data.drop(train_data.columns[drop_index], axis=1, inplace=True)

# fork from https://www.kaggle.com/jazivxt/fun-with-ms-dataset

gf_defaults = {'col': [], 'ocol': [],
               'dcol': ['EngineVersion', 'AppVersion', 'AvSigVersion', 'OsBuildLab', 'Census_OSVersion']}
one_hot = {}


def get_features(df, gf_train=False):
    global one_hot
    global gf_defaults

    for c in gf_defaults['dcol']:
        for i in range(5):
            df[c + str(i)] = df[c].map(lambda x: str(x).split('.')[i] if len(str(x).split('.')) > i else -1)

    col = [c for c in df.columns if c not in ['MachineIdentifier', 'HasDetections']]
    if gf_train:
        for c in col:
            if df[c].dtype == 'O' or df[c].dtype.name == 'category' or df[c].dtype == 'object' or df[
                c].dtype.name == 'object':
                gf_defaults['ocol'].append(c)
            else:
                gf_defaults['col'].append(c)
        one_hot = {c: list(df[c].value_counts().index) for c in gf_defaults['ocol']}

    # train and test
    for c in one_hot:
        if len(one_hot[c]) > 1 and len(one_hot[c]) < 20:
            for val in one_hot[c]:
                df[c + '_oh_' + str(val)] = (df[c].values == val).astype(np.int)
                if gf_train:
                    gf_defaults['col'].append(c + '_oh_' + str(val))
    return df[gf_defaults['col'] + ['MachineIdentifier', 'HasDetections']]


col = gf_defaults['col']
model = []
params = {'objective': 'binary', "boosting": "gbdt", 'learning_rate': 0.02, 'max_depth': -1,
          "feature_fraction": 0.8, "bagging_freq": 1, "bagging_fraction": 0.8, "bagging_seed": 11,
          "metric": 'auc', "lambda_l1": 0.1, 'num_leaves': 60, 'min_data_in_leaf': 60, "verbosity": -1,
          "random_state": 3}
online_start = True
for df in train_data:
    if online_start:
        df = get_features(df, True)
        x1, x2, y1, y2 = model_selection.train_test_split(df[col], df['HasDetections'], test_size=0.2, random_state=25)
        model = lgb.train(params, lgb.Dataset(x1, y1), 2500, lgb.Dataset(x2, y2), verbose_eval=100,
                          early_stopping_rounds=200)
        model.save_model('lgb.model')
    else:
        df = get_features(df)
        x1, x2, y1, y2 = model_selection.train_test_split(df[col], df['HasDetections'], test_size=0.2, random_state=25)
        model = lgb.train(params, lgb.Dataset(x1, y1), 2500, lgb.Dataset(x2, y2), verbose_eval=100,
                          early_stopping_rounds=200, init_model='lgb.model')
        model.save_model('lgb.model')
    online_start = False
    print('training...')

predictions = []
for df in test_data:
    df['HasDetections'] = 0.0
    df = get_features(df)
    df['HasDetections'] = model.predict(df[col], num_iteration=model.best_iteration + 50)
    predictions.append(df[['MachineIdentifier', 'HasDetections']].values)
    print('testing...')

sub = np.concatenate(predictions)
sub = pd.DataFrame(sub, columns=['MachineIdentifier', 'HasDetections'])
sub.to_csv('submission.csv', index=False)
